{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word_embedding\n",
    "\n",
    "Word embedding, bir kelime veya ifade için düz bir vektör olarak ifade edilmesidir. Bu vektörler, kelimelerin anlamları arasındaki benzerlikleri yansıtmak üzere öğrenilir. Bu sayede, yapay zeka modelleri kelimeler arasındaki ilişkileri daha iyi anlayabilir\n",
    "\n",
    "Word embedding, metin verileri kullanılarak öğrenilir. Örneğin, bir metin corpus'u (büyük metin veritabanı) kullanılarak, her kelime için çevresindeki kelimeler ile ilişkisini öğrenir. Bu ilişki, kelime ve çevresindeki kelimeler arasındaki co-occurrence (ortak görünüm) frekansı olarak tanımlanır. Daha sonra, bu co-occurrence verileri kullanarak, her kelime için bir vektör oluşturulur. Bu vektörler, kelimelerin anlamları arasındaki benzerlikleri yansıtmak üzere tasarlanır.\n",
    "\n",
    "Örneğin, \"öğretmen\", \"öğrenci\", \"not verme\" kelimeleri sıklıkla aynı metinlerde görülür ve benzer anlamlar taşır, bu nedenle oluşan vektörleri birbirine yakın olacak. Ancak \"öğretmen\" ve \"portakal\" kelimeleri sıklıkla aynı metinlerde görülmez ve farklı anlamlar taşır, bu nedenle oluşan vektörleri birbirinden uzak olacak."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Representation (Feature Extraction for word embeddings)\n",
    "\n",
    "word embbeding vektöründe yeralan ve genellikle -1 ve +1 (Bazı pre-trained modellerde bu değer -1 ve +1'den büyük olabilir.) arasında değer alan elemanların her birine feature representation denir. Bu feature representationların model tarafından oluşturulması sürecisine ise feature extraction denir.\n",
    "\n",
    "ML'de, feature extraction, veri kümesinde bulunan özellikleri veya nitelikleri belirlemek için kullanılan bir yöntemdir. Bu özellikler, veri kümesindeki verileri anlamlı bir şekilde ifade etmelerine olanak tanır. Bu özellikler, veri kümesinden manuel olarak seçilir.\n",
    "\n",
    "DL'de ise, feature extraction, veri kümesindeki özellikleri öğrenmek için kullanılan bir yöntemdir. Bu yöntem için kullanılan yöntemler arasında Convolutional Neural Network (CNN) ve Recurrent Neural Network (RNN) gibi yapay sinir ağları yer almaktadır. Bu yapay sinir ağları, veri kümesinden özellikleri otomatik olarak öğrenir ve bu özellikler, veri kümesindeki verileri daha anlamlı bir şekilde ifade etmelerine olanak tanır.\n",
    "\n",
    "Özet olarak, ML'de özellikler manuel olarak kullanıcı tarafından seçilirken, DL'de özellikler otomatik model tarafından tespit edilerek öğrenilir."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gensim in c:\\users\\insan\\anaconda3\\lib\\site-packages (4.1.2)\n",
      "Requirement already satisfied: numpy>=1.17.0 in c:\\users\\insan\\appdata\\roaming\\python\\python39\\site-packages (from gensim) (1.23.5)\n",
      "Requirement already satisfied: scipy>=0.18.1 in c:\\users\\insan\\anaconda3\\lib\\site-packages (from gensim) (1.9.1)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in c:\\users\\insan\\anaconda3\\lib\\site-packages (from gensim) (5.2.1)\n"
     ]
    }
   ],
   "source": [
    "#!pip install gensim\n",
    "# conda install -c conda-forge gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "import pandas as pd\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>news</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>iran devlet televizyonu ülkedeki eyaletin sind...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>gösterilerde fitnecilere ölüm münafıklara ölüm...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>dini lider ali hamaney ve cumhurbaşkanı mahmud...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>musevi ye ölüm ve idam idam sloganları duyuldu</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>muhalefet liderleri kaçtı mı aşure günü yaşana...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>411520</th>\n",
       "      <td>dışişleri bakanlığı ndan yapılan yazılı açıkla...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>411521</th>\n",
       "      <td>açıklamada abd nin ankara büyükelçiliği ve ist...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>411522</th>\n",
       "      <td>seyahat uyarısı güncelleme kararının temmuz da...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>411523</th>\n",
       "      <td>amerikalı turistlerin açıkça türkiye deki ulus...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>411524</th>\n",
       "      <td>abd dışişleri bakanlığı seyahat uyarısı açıkla...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>411525 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     news\n",
       "0       iran devlet televizyonu ülkedeki eyaletin sind...\n",
       "1       gösterilerde fitnecilere ölüm münafıklara ölüm...\n",
       "2       dini lider ali hamaney ve cumhurbaşkanı mahmud...\n",
       "3         musevi ye ölüm ve idam idam sloganları duyuldu \n",
       "4       muhalefet liderleri kaçtı mı aşure günü yaşana...\n",
       "...                                                   ...\n",
       "411520  dışişleri bakanlığı ndan yapılan yazılı açıkla...\n",
       "411521  açıklamada abd nin ankara büyükelçiliği ve ist...\n",
       "411522  seyahat uyarısı güncelleme kararının temmuz da...\n",
       "411523  amerikalı turistlerin açıkça türkiye deki ulus...\n",
       "411524  abd dışişleri bakanlığı seyahat uyarısı açıkla...\n",
       "\n",
       "[411525 rows x 1 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('newspaper.zip', names = [\"news\"])\n",
    "df\n",
    "\n",
    "# pandas'ın read_csv fonksiyonu zip'li dosyaları da okuyabiliyor. \n",
    "# ziplediğimiz data(corpus) txt dosyası olduğundan dosyada belirlenmiş feture ismi/isimleri belirlenmemiştir. Bu sebeple \n",
    "# feature isimlendirmesi için names parametresini kullanıyoruz.\n",
    "# names parametresi kullanılmaz ise ilk satırdaki text default olarak feature names olarak atanır."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'saydım duş almak için üç dakika yeterli ve bakın kokmuyorum da '"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.news[41]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['saydım',\n",
       " 'duş',\n",
       " 'almak',\n",
       " 'için',\n",
       " 'üç',\n",
       " 'dakika',\n",
       " 'yeterli',\n",
       " 've',\n",
       " 'bakın',\n",
       " 'kokmuyorum',\n",
       " 'da']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_tokenize(df.news[41])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['iran', 'devlet', 'televizyonu', 'ülkedeki', 'eyaletin', 'sinde', 'yapılan', 'reformcuları', 'protesto', 'amaçlı', 'yürüyüşlere', 'milyonlarca', 'kişinin', 'katıldığını', 'bildirdi'], ['gösterilerde', 'fitnecilere', 'ölüm', 'münafıklara', 'ölüm', 'abd', 'ye', 'ölüm', 'ingiltere', 'ye', 'ölüm', 'sloganları', 'atıldı'], ['dini', 'lider', 'ali', 'hamaney', 've', 'cumhurbaşkanı', 'mahmud', 'ahmedinejad', 'ı', 'destekleyen', 'iranlılar', 'son', 'olaylarda', 'yeğeni', 'öldürülen', 'mir', 'hüseyin', 'musevi', 'başta', 'olmak', 'üzere', 'muhalefet', 'liderlerini', 'kınadılar'], ['musevi', 'ye', 'ölüm', 've', 'idam', 'idam', 'sloganları', 'duyuldu'], ['muhalefet', 'liderleri', 'kaçtı', 'mı', 'aşure', 'günü', 'yaşanan', 'çatışmalarda', 'devlet', 'kaynaklarına', 'göre', 'u', 'terörist', 'olmak', 'üzere', 'kişi', 'ölmüştü']]\n"
     ]
    }
   ],
   "source": [
    "corpus = []\n",
    "\n",
    "for i in df.news:\n",
    "    corpus.append(word_tokenize(i))\n",
    "\n",
    "print(corpus[:5])\n",
    "\n",
    "# word2vec algoritması tüm corpusun 2 boyutlu olmasını ister. Bu sebeple burdaki for döngüsü ile tüm documnetleri/satırları\n",
    "# teker teker çekip word tokenlerine ayırıyoruz. Word_tokenize fonksiyonu default olarak texti word tokenlerine ayırıp bir \n",
    "# listeye atar. Bu listeler corpus listesine append edilerek corpus 2 boyutlu hale getiritir. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Word2Vec(corpus, vector_size=100, window=5, min_count=5, sg=1)\n",
    "\n",
    "# vector_size, kaç boyutlu word embeddings istiyorsak burda belirtiyoruz.\n",
    "# Bir token ile diğer tokenler arasındaki anlamsal ilişkiler kurulurken bu tokenin kaç önce ve sonrasındaki tokenler dikkate \n",
    "# alınarak eğitim yapılmasını istiyorsak window parametresinde belirtiyoruz. Tavsiye edilen 5-15 sayıları arasındadır.\n",
    "# min_count, corpusta 5 veya daha az geçen tokenler eğitime dahil edilmez. Genellikle 3,4,5 gibi sayılar tercih edilir.\n",
    "# sg =1, eğitimi skipgram algoritması ile yap.\n",
    "# sg =0, eğitimi CBOW algoritması ile yap."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.31081417, -0.12164167,  0.19561437,  0.41136748,  0.32842496,\n",
       "        0.11194248, -0.56829304,  0.5661261 , -0.11081512, -0.36489815,\n",
       "       -0.07611699, -0.6496909 , -0.41072485,  0.03208696,  0.00328285,\n",
       "        0.6324867 ,  0.09752325, -0.4805997 ,  0.11841419, -1.0365313 ,\n",
       "       -0.23365067,  0.37326235,  0.6060967 , -0.5856859 ,  0.08865505,\n",
       "        0.2589555 , -0.14632373,  0.05221381, -0.09896757,  0.70737904,\n",
       "        0.49006   ,  0.10280373,  0.0680707 , -0.50594646,  0.04187351,\n",
       "       -0.31932864, -0.09711969, -0.40514132,  0.24382436, -0.40366608,\n",
       "        0.2768256 ,  0.03208508,  0.5084667 , -0.05990181,  0.5464565 ,\n",
       "        0.4017679 ,  0.01139939, -0.10083386,  0.06598069, -0.22588393,\n",
       "        0.42154974, -0.07887193, -0.0342469 , -0.19730426, -0.1070734 ,\n",
       "       -0.13766377,  0.01543441, -0.10659543, -0.37855142, -0.5384031 ,\n",
       "       -0.03633417, -0.25605962,  0.3221969 ,  0.15415335, -0.769064  ,\n",
       "        0.09362946,  0.07979269,  0.27179164, -0.03526044, -0.10299186,\n",
       "       -0.4787822 , -0.25673807,  0.4506583 , -0.01441643,  0.3624137 ,\n",
       "        0.13096881,  0.27904624, -0.06995494, -0.62352586,  0.04716529,\n",
       "        0.14514256, -0.5933691 , -0.24385183,  0.3674711 , -0.14285953,\n",
       "        0.2097073 ,  0.07380136,  0.06460885,  0.7727002 ,  0.3036484 ,\n",
       "        0.35849804,  0.23048806,  0.17495464,  0.05357121,  0.00710706,\n",
       "       -0.14192747,  0.33985895,  0.2904405 ,  0.40413377, -0.2306218 ],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv['ankara']\n",
    "\n",
    "# 100 elemanlı/boyutlu word_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('boynundan', 0.8018431663513184),\n",
       " ('bıçağı', 0.7906331419944763),\n",
       " ('arabasında', 0.7662729024887085),\n",
       " ('yelek', 0.765421450138092),\n",
       " ('balta', 0.7649369239807129),\n",
       " ('darbeleriyle', 0.7606822848320007),\n",
       " ('darbelerine', 0.759668231010437),\n",
       " ('morluklar', 0.7593773603439331),\n",
       " ('kılıçla', 0.7557517290115356),\n",
       " ('yelekler', 0.752866804599762)]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.most_similar('bıçak')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('gülen', 0.9113976955413818),\n",
       " ('fetullah', 0.817703902721405),\n",
       " ('vaiz', 0.7760117650032043),\n",
       " ('elebaşı', 0.7521460652351379),\n",
       " ('gülerce', 0.6823400855064392),\n",
       " ('fuller', 0.6784144043922424),\n",
       " ('cemaati', 0.6698505282402039),\n",
       " ('dershane', 0.6651429533958435),\n",
       " ('iadesiyle', 0.6538774967193604),\n",
       " ('adl', 0.6522930860519409)]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.most_similar('nur')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('evine', 0.7999204993247986),\n",
       " ('dükkana', 0.7542088627815247),\n",
       " ('apartmana', 0.7303590774536133),\n",
       " ('mağazaya', 0.7222842574119568),\n",
       " ('arabasına', 0.720339834690094),\n",
       " ('komşusunun', 0.6998159885406494),\n",
       " ('okula', 0.6950918436050415),\n",
       " ('arabaya', 0.694274365901947),\n",
       " ('karakola', 0.6926326155662537),\n",
       " ('lokantaya', 0.6886556148529053)]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.most_similar('eve')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('marmara', 0.8920373320579529),\n",
       " ('gemisine', 0.6956965327262878),\n",
       " ('baskınıyla', 0.6879400014877319),\n",
       " ('dökme', 0.6312839388847351),\n",
       " ('baskınına', 0.6210713982582092),\n",
       " ('filosundaki', 0.6163733601570129),\n",
       " ('saldırısındaki', 0.6150459051132202),\n",
       " ('filodaki', 0.6019846200942993),\n",
       " ('gemisindeki', 0.5971487760543823),\n",
       " ('baskınının', 0.5902086496353149)]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.most_similar('mavi')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('dersin', 0.655022382736206),\n",
       " ('eğitimciler', 0.6529861092567444),\n",
       " ('imamlara', 0.6514789462089539),\n",
       " ('driscoll', 0.6504199504852295),\n",
       " ('eczacı', 0.6480433344841003)]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.most_similar(positive=['öğrenme', 'doktor'], negative=['tedavi'], topn=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('hollanda', 0.6443963646888733)]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.most_similar(positive=['ankara', 'belçika'], negative=['brüksel'], topn=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"word2vec.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Word2Vec.load(\"word2vec.model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Glove"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "\n",
    "# KeyedVectors fonksiyonunu farklı bir formattaki word embeddingleri word2vec formatına dönüştürmek için kullanıyoruz."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "could not convert string to float: 'ORWELL'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_10860\\3682105399.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mglove_model\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'hayvanciftligi.txt'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mmodel2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mKeyedVectors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload_word2vec_format\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mglove_model\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mno_header\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m#'glove.6B.100d.txt'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;31m# 'Word2Vec formatında genellikle dosyanın ilk satırında bir başlık bulunur ve bu başlık satırı, kelime sayısını\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;31m# ve vektör boyutunu içerir. Ancak txt dosyamız glove formatında olduğundan glove.txt dosyalarında başlık bulunmaz\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\gensim\\models\\keyedvectors.py\u001b[0m in \u001b[0;36mload_word2vec_format\u001b[1;34m(cls, fname, fvocab, binary, encoding, unicode_errors, limit, datatype, no_header)\u001b[0m\n\u001b[0;32m   1627\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1628\u001b[0m         \"\"\"\n\u001b[1;32m-> 1629\u001b[1;33m         return _load_word2vec_format(\n\u001b[0m\u001b[0;32m   1630\u001b[0m             \u001b[0mcls\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfvocab\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfvocab\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbinary\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbinary\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mencoding\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0municode_errors\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0municode_errors\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1631\u001b[0m             \u001b[0mlimit\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlimit\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdatatype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdatatype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mno_header\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mno_header\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\gensim\\models\\keyedvectors.py\u001b[0m in \u001b[0;36m_load_word2vec_format\u001b[1;34m(cls, fname, fvocab, binary, encoding, unicode_errors, limit, datatype, no_header, binary_chunk_size)\u001b[0m\n\u001b[0;32m   1959\u001b[0m                 \u001b[1;32mraise\u001b[0m \u001b[0mNotImplementedError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"no_header only available for text-format files\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1960\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# text\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1961\u001b[1;33m                 \u001b[0mvocab_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvector_size\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_word2vec_detect_sizes_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfin\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlimit\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdatatype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0municode_errors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1962\u001b[0m             \u001b[0mfin\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1963\u001b[0m             \u001b[0mfin\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mutils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'rb'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\gensim\\models\\keyedvectors.py\u001b[0m in \u001b[0;36m_word2vec_detect_sizes_text\u001b[1;34m(fin, limit, datatype, unicode_errors, encoding)\u001b[0m\n\u001b[0;32m   1897\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mvector_size\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1898\u001b[0m             \u001b[1;32mcontinue\u001b[0m  \u001b[1;31m# don't bother parsing lines past the 1st\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1899\u001b[1;33m         \u001b[0mword\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweights\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_word2vec_line_to_vector\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mline\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdatatype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0municode_errors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1900\u001b[0m         \u001b[0mvector_size\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mweights\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1901\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mvocab_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvector_size\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\gensim\\models\\keyedvectors.py\u001b[0m in \u001b[0;36m_word2vec_line_to_vector\u001b[1;34m(line, datatype, unicode_errors, encoding)\u001b[0m\n\u001b[0;32m   1885\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0m_word2vec_line_to_vector\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mline\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdatatype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0municode_errors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1886\u001b[0m     \u001b[0mparts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mutils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_unicode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mline\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrstrip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mencoding\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0municode_errors\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\" \"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1887\u001b[1;33m     \u001b[0mword\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweights\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mparts\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mdatatype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mparts\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1888\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mword\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweights\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1889\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\gensim\\models\\keyedvectors.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m   1885\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0m_word2vec_line_to_vector\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mline\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdatatype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0municode_errors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1886\u001b[0m     \u001b[0mparts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mutils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_unicode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mline\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrstrip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mencoding\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0municode_errors\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\" \"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1887\u001b[1;33m     \u001b[0mword\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweights\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mparts\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mdatatype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mparts\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1888\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mword\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweights\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1889\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: could not convert string to float: 'ORWELL'"
     ]
    }
   ],
   "source": [
    "glove_model = 'hayvanciftligi.txt'\n",
    "model2 = KeyedVectors.load_word2vec_format(glove_model, no_header=True) #'glove.6B.100d.txt'\n",
    "\n",
    "# 'Word2Vec formatında genellikle dosyanın ilk satırında bir başlık bulunur ve bu başlık satırı, kelime sayısını\n",
    "# ve vektör boyutunu içerir. Ancak txt dosyamız glove formatında olduğundan glove.txt dosyalarında başlık bulunmaz\n",
    "# başlık bilgisi olmadığını no_header=True şeklinde belirtmemiz gerekir aksi taktirde hata alırsınız."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.44374 ,  0.67311 , -0.51096 ,  0.20882 , -0.10662 ,  0.55098 ,\n",
       "       -0.035593,  0.25126 , -0.32789 ,  1.0762  , -0.49637 , -0.4298  ,\n",
       "        0.36764 ,  0.57894 , -0.25027 , -0.41021 ,  0.086998, -0.16843 ,\n",
       "       -0.85764 ,  1.0404  , -1.0314  ,  0.095147,  0.30729 ,  0.12348 ,\n",
       "        0.22745 , -0.52157 , -0.72478 , -1.0843  ,  0.035966,  0.62985 ,\n",
       "       -1.0991  ,  0.67161 ,  0.33797 ,  0.14551 , -0.90049 , -0.064415,\n",
       "       -0.75247 ,  0.21741 ,  0.51594 , -0.46291 , -0.77598 ,  0.40705 ,\n",
       "        0.1889  , -0.43402 ,  0.23202 , -0.081453, -0.3882  , -0.34444 ,\n",
       "        0.080225, -0.28274 , -0.38869 , -0.58152 , -0.25558 ,  1.0027  ,\n",
       "       -0.11114 , -1.5402  , -0.16761 , -0.26558 ,  0.9325  ,  0.069397,\n",
       "        0.96618 ,  0.15449 , -0.22905 , -0.1761  ,  0.13225 , -0.55741 ,\n",
       "        0.9234  , -0.04845 ,  0.50202 ,  1.0144  , -0.1256  ,  0.30486 ,\n",
       "        0.090808,  0.17642 , -0.23146 ,  0.68386 ,  0.37269 , -0.37316 ,\n",
       "       -0.025728, -1.0279  , -0.33142 ,  0.036028, -0.24925 , -1.4405  ,\n",
       "       -1.6267  ,  0.082284, -0.080153, -0.50802 ,  0.031885, -0.60546 ,\n",
       "        0.2908  ,  0.036842,  0.55328 ,  0.66784 , -0.42574 ,  0.53331 ,\n",
       "        0.053644, -0.66522 , -0.10012 , -0.17729 ], dtype=float32)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2['teacher']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('turkey', 0.7512096762657166),\n",
       " ('istanbul', 0.6787630319595337),\n",
       " ('turkish', 0.6690374612808228),\n",
       " ('damascus', 0.6372509002685547),\n",
       " ('tbilisi', 0.6322181820869446),\n",
       " ('erdogan', 0.6258037090301514),\n",
       " ('moscow', 0.6217040419578552),\n",
       " ('brussels', 0.6181439161300659),\n",
       " ('skopje', 0.6164302825927734),\n",
       " ('cyprus', 0.6064029932022095)]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2.most_similar('ankara')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('student', 0.8083399534225464),\n",
       " ('school', 0.7545564770698547),\n",
       " ('teaching', 0.7521439790725708),\n",
       " ('taught', 0.741184651851654),\n",
       " ('teachers', 0.7291542887687683),\n",
       " ('graduate', 0.7134960293769836),\n",
       " ('instructor', 0.7077120542526245),\n",
       " ('students', 0.6828974485397339),\n",
       " ('teaches', 0.6552315354347229),\n",
       " ('education', 0.6528989672660828)]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2.most_similar('teacher')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('physician', 0.7673239707946777),\n",
       " ('nurse', 0.75215083360672),\n",
       " ('dr.', 0.7175194025039673),\n",
       " ('doctors', 0.7080884575843811),\n",
       " ('patient', 0.7074184417724609),\n",
       " ('medical', 0.6995992660522461),\n",
       " ('surgeon', 0.6905338168144226),\n",
       " ('hospital', 0.6900930404663086),\n",
       " ('psychiatrist', 0.658909797668457),\n",
       " ('dentist', 0.6447421312332153)]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2.most_similar('doctor')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('daughter', 0.9090957045555115)]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2.most_similar(positive=['woman', 'son'], negative=['man'], topn=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('mother', 0.9024619460105896)]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2.most_similar(positive=['woman', 'father'], negative=['man'], topn=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('aunt', 0.836803138256073)]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2.most_similar(positive=['woman', 'uncle'], negative=['man'], topn=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('turkey', 0.81471186876297)]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2.most_similar(positive=['ankara', 'germany'], negative=['berlin'], topn=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('teacher', 0.7610154151916504)]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2.most_similar(positive=['teach', 'doctor'], negative=['treat'], topn=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('queen', 0.7698541283607483)]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2.most_similar(positive=['woman', 'king'], negative=['man'], topn=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('lover', 0.7032662630081177)]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2.most_similar(positive=['love', 'jealous'], negative=['hate'], topn=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
